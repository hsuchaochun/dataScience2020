{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import datetime\n",
    "import time\n",
    "from selenium import webdriver\n",
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "import re\n",
    "from multiprocessing import Pool\n",
    "import argparse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<!DOCTYPE html>\n",
      "<html>\n",
      "\t<head>\n",
      "\t\t<meta charset=\"utf-8\">\n",
      "\t\t\n",
      "\n",
      "<meta name=\"viewport\" content=\"width=device-width, initial-scale=1\">\n",
      "\n",
      "<title>看板 Beauty 文章列表 - 批踢踢實業坊</title>\n",
      "\n",
      "<link rel=\"stylesheet\" type=\"text/css\" href=\"//images.ptt.cc/bbs/v2.27/bbs-common.css\">\n",
      "<link rel=\"stylesheet\" type=\"text/css\" href=\"//images.ptt.cc/bbs/v2.27/bbs-base.css\" media=\"screen\">\n",
      "<link rel=\"stylesheet\" type=\"text/css\" href=\"//images.ptt.cc/bbs/v2.27/bbs-custom.css\">\n",
      "<link rel=\"stylesheet\" type=\"text/css\" href=\"//images.ptt.cc/bbs/v2.27/pushstream.css\" media=\"screen\">\n",
      "<link rel=\"stylesheet\" type=\"text/css\" href=\"//images.ptt.cc/bbs/v2.27/bbs-print.css\" media=\"print\">\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\t</head>\n",
      "    <body>\n",
      "\t\t\n",
      "<div id=\"topbar-container\">\n",
      "\t<div id=\"topbar\" class=\"bbs-content\">\n",
      "\t\t<a id=\"logo\" href=\"/bbs/\">批踢踢實業坊</a>\n",
      "\t\t<span>&rsaquo;</span>\n",
      "\t\t<a class=\"board\" href=\"/bbs/Beauty/index.html\"><span class=\"board-label\">看板 </span>Beauty</a>\n",
      "\t\t<a class=\"right small\" href=\"/about.html\">關於我們</a>\n",
      "\t\t<a class=\"right small\" href=\"/contact.html\">聯絡資訊</a>\n",
      "\t</div>\n",
      "</div>\n",
      "\n",
      "<div id=\"main-container\">\n",
      "\t<div id=\"action-bar-container\">\n",
      "\t\t<div class=\"action-bar\">\n",
      "\t\t\t<div class=\"btn-group btn-group-dir\">\n",
      "\t\t\t\t<a class=\"btn selected\" href=\"/bbs/Beauty/index.html\">看板</a>\n",
      "\t\t\t\t<a class=\"btn\" href=\"/man/Beauty/index.html\">精華區</a>\n",
      "\t\t\t</div>\n",
      "\t\t\t<div class=\"btn-group btn-group-paging\">\n",
      "\t\t\t\t<a class=\"btn wide\" href=\"/bbs/Beauty/index1.html\">最舊</a>\n",
      "\t\t\t\t<a class=\"btn wide\" href=\"/bbs/Beauty/index3450.html\">&lsaquo; 上頁</a>\n",
      "\t\t\t\t<a class=\"btn wide disabled\">下頁 &rsaquo;</a>\n",
      "\t\t\t\t<a class=\"btn wide\" href=\"/bbs/Beauty/index.html\">最新</a>\n",
      "\t\t\t</div>\n",
      "\t\t</div>\n",
      "\t</div>\n",
      "\n",
      "\t<div class=\"r-list-container action-bar-margin bbs-screen\">\n",
      "\t\t<div class=\"search-bar\">\n",
      "\t\t\t<form type=\"get\" action=\"search\" id=\"search-bar\">\n",
      "\t\t\t\t<input class=\"query\" type=\"text\" name=\"q\" value=\"\" placeholder=\"搜尋文章&#x22ef;\">\n",
      "\t\t\t</form>\n",
      "\t\t</div>\n",
      "\n",
      "\t\t\n",
      "\t\t\n",
      "            \n",
      "        \n",
      "        \n",
      "\t\t<div class=\"r-ent\">\n",
      "\t\t\t<div class=\"nrec\"></div>\n",
      "\t\t\t<div class=\"title\">\n",
      "\t\t\t\n",
      "\t\t\t\t<a href=\"/bbs/Beauty/M.1602393476.A.23C.html\">[正妹] 可愛笑容的體操少女</a>\n",
      "\t\t\t\n",
      "\t\t\t</div>\n",
      "\t\t\t<div class=\"meta\">\n",
      "\t\t\t\t<div class=\"author\">HarunaOno</div>\n",
      "\t\t\t\t<div class=\"article-menu\">\n",
      "\t\t\t\t\t\n",
      "\t\t\t\t\t<div class=\"trigger\">&#x22ef;</div>\n",
      "\t\t\t\t\t<div class=\"dropdown\">\n",
      "\t\t\t\t\t\t<div class=\"item\"><a href=\"/bbs/Beauty/search?q=thread%3A%5B%E6%AD%A3%E5%A6%B9%5D&#43;%E5%8F%AF%E6%84%9B%E7%AC%91%E5%AE%B9%E7%9A%84%E9%AB%94%E6%93%8D%E5%B0%91%E5%A5%B3\">搜尋同標題文章</a></div>\n",
      "\t\t\t\t\t\t\n",
      "\t\t\t\t\t\t<div class=\"item\"><a href=\"/bbs/Beauty/search?q=author%3AHarunaOno\">搜尋看板內 HarunaOno 的文章</a></div>\n",
      "\t\t\t\t\t\t\n",
      "\t\t\t\t\t</div>\n",
      "\t\t\t\t\t\n",
      "\t\t\t\t</div>\n",
      "\t\t\t\t<div class=\"date\">10/11</div>\n",
      "\t\t\t\t<div class=\"mark\"></div>\n",
      "\t\t\t</div>\n",
      "\t\t</div>\n",
      "\n",
      "\t\t\n",
      "        \n",
      "        <div class=\"r-list-sep\"></div>\n",
      "            \n",
      "                \n",
      "        \n",
      "        \n",
      "\t\t<div class=\"r-ent\">\n",
      "\t\t\t<div class=\"nrec\"><span class=\"hl f3\">40</span></div>\n",
      "\t\t\t<div class=\"title\">\n",
      "\t\t\t\n",
      "\t\t\t\t<a href=\"/bbs/Beauty/M.1589282607.A.C87.html\">[公告] 不願上表特 ＆ 優文推薦 ＆ 檢舉建議專區</a>\n",
      "\t\t\t\n",
      "\t\t\t</div>\n",
      "\t\t\t<div class=\"meta\">\n",
      "\t\t\t\t<div class=\"author\">cu0920</div>\n",
      "\t\t\t\t<div class=\"article-menu\">\n",
      "\t\t\t\t\t\n",
      "\t\t\t\t\t<div class=\"trigger\">&#x22ef;</div>\n",
      "\t\t\t\t\t<div class=\"dropdown\">\n",
      "\t\t\t\t\t\t<div class=\"item\"><a href=\"/bbs/Beauty/search?q=thread%3A%5B%E5%85%AC%E5%91%8A%5D&#43;%E4%B8%8D%E9%A1%98%E4%B8%8A%E8%A1%A8%E7%89%B9&#43;%EF%BC%86&#43;%E5%84%AA%E6%96%87%E6%8E%A8%E8%96%A6&#43;%EF%BC%86&#43;%E6%AA%A2%E8%88%89%E5%BB%BA%E8%AD%B0%E5%B0%88%E5%8D%80\">搜尋同標題文章</a></div>\n",
      "\t\t\t\t\t\t\n",
      "\t\t\t\t\t\t<div class=\"item\"><a href=\"/bbs/Beauty/search?q=author%3Acu0920\">搜尋看板內 cu0920 的文章</a></div>\n",
      "\t\t\t\t\t\t\n",
      "\t\t\t\t\t</div>\n",
      "\t\t\t\t\t\n",
      "\t\t\t\t</div>\n",
      "\t\t\t\t<div class=\"date\"> 5/12</div>\n",
      "\t\t\t\t<div class=\"mark\">M</div>\n",
      "\t\t\t</div>\n",
      "\t\t</div>\n",
      "\n",
      "            \n",
      "                \n",
      "        \n",
      "        \n",
      "\t\t<div class=\"r-ent\">\n",
      "\t\t\t<div class=\"nrec\"></div>\n",
      "\t\t\t<div class=\"title\">\n",
      "\t\t\t\n",
      "\t\t\t\t<a href=\"/bbs/Beauty/M.1563960846.A.05A.html\">Fw: [公告] 請使用者多加注意我國保護兒少的法令</a>\n",
      "\t\t\t\n",
      "\t\t\t</div>\n",
      "\t\t\t<div class=\"meta\">\n",
      "\t\t\t\t<div class=\"author\">hateOnas</div>\n",
      "\t\t\t\t<div class=\"article-menu\">\n",
      "\t\t\t\t\t\n",
      "\t\t\t\t\t<div class=\"trigger\">&#x22ef;</div>\n",
      "\t\t\t\t\t<div class=\"dropdown\">\n",
      "\t\t\t\t\t\t<div class=\"item\"><a href=\"/bbs/Beauty/search?q=thread%3A%5B%E5%85%AC%E5%91%8A%5D&#43;%E8%AB%8B%E4%BD%BF%E7%94%A8%E8%80%85%E5%A4%9A%E5%8A%A0%E6%B3%A8%E6%84%8F%E6%88%91%E5%9C%8B%E4%BF%9D%E8%AD%B7%E5%85%92%E5%B0%91%E7%9A%84%E6%B3%95%E4%BB%A4\">搜尋同標題文章</a></div>\n",
      "\t\t\t\t\t\t\n",
      "\t\t\t\t\t\t<div class=\"item\"><a href=\"/bbs/Beauty/search?q=author%3AhateOnas\">搜尋看板內 hateOnas 的文章</a></div>\n",
      "\t\t\t\t\t\t\n",
      "\t\t\t\t\t</div>\n",
      "\t\t\t\t\t\n",
      "\t\t\t\t</div>\n",
      "\t\t\t\t<div class=\"date\"> 7/24</div>\n",
      "\t\t\t\t<div class=\"mark\">!</div>\n",
      "\t\t\t</div>\n",
      "\t\t</div>\n",
      "\n",
      "            \n",
      "                \n",
      "        \n",
      "        \n",
      "\t\t<div class=\"r-ent\">\n",
      "\t\t\t<div class=\"nrec\"></div>\n",
      "\t\t\t<div class=\"title\">\n",
      "\t\t\t\n",
      "\t\t\t\t<a href=\"/bbs/Beauty/M.1564114881.A.155.html\">[公告] 表特板板規(2020.3.24)</a>\n",
      "\t\t\t\n",
      "\t\t\t</div>\n",
      "\t\t\t<div class=\"meta\">\n",
      "\t\t\t\t<div class=\"author\">hateOnas</div>\n",
      "\t\t\t\t<div class=\"article-menu\">\n",
      "\t\t\t\t\t\n",
      "\t\t\t\t\t<div class=\"trigger\">&#x22ef;</div>\n",
      "\t\t\t\t\t<div class=\"dropdown\">\n",
      "\t\t\t\t\t\t<div class=\"item\"><a href=\"/bbs/Beauty/search?q=thread%3A%5B%E5%85%AC%E5%91%8A%5D&#43;%E8%A1%A8%E7%89%B9%E6%9D%BF%E6%9D%BF%E8%A6%8F%282020.3.24%29\">搜尋同標題文章</a></div>\n",
      "\t\t\t\t\t\t\n",
      "\t\t\t\t\t\t<div class=\"item\"><a href=\"/bbs/Beauty/search?q=author%3AhateOnas\">搜尋看板內 hateOnas 的文章</a></div>\n",
      "\t\t\t\t\t\t\n",
      "\t\t\t\t\t</div>\n",
      "\t\t\t\t\t\n",
      "\t\t\t\t</div>\n",
      "\t\t\t\t<div class=\"date\"> 7/26</div>\n",
      "\t\t\t\t<div class=\"mark\">!</div>\n",
      "\t\t\t</div>\n",
      "\t\t</div>\n",
      "\n",
      "            \n",
      "                \n",
      "        \n",
      "        \n",
      "\t\t<div class=\"r-ent\">\n",
      "\t\t\t<div class=\"nrec\"><span class=\"hl f3\">17</span></div>\n",
      "\t\t\t<div class=\"title\">\n",
      "\t\t\t\n",
      "\t\t\t\t<a href=\"/bbs/Beauty/M.1574513001.A.A80.html\">[公告] 請勿意淫推文 </a>\n",
      "\t\t\t\n",
      "\t\t\t</div>\n",
      "\t\t\t<div class=\"meta\">\n",
      "\t\t\t\t<div class=\"author\">hateOnas</div>\n",
      "\t\t\t\t<div class=\"article-menu\">\n",
      "\t\t\t\t\t\n",
      "\t\t\t\t\t<div class=\"trigger\">&#x22ef;</div>\n",
      "\t\t\t\t\t<div class=\"dropdown\">\n",
      "\t\t\t\t\t\t<div class=\"item\"><a href=\"/bbs/Beauty/search?q=thread%3A%5B%E5%85%AC%E5%91%8A%5D&#43;%E8%AB%8B%E5%8B%BF%E6%84%8F%E6%B7%AB%E6%8E%A8%E6%96%87&#43;\">搜尋同標題文章</a></div>\n",
      "\t\t\t\t\t\t\n",
      "\t\t\t\t\t\t<div class=\"item\"><a href=\"/bbs/Beauty/search?q=author%3AhateOnas\">搜尋看板內 hateOnas 的文章</a></div>\n",
      "\t\t\t\t\t\t\n",
      "\t\t\t\t\t</div>\n",
      "\t\t\t\t\t\n",
      "\t\t\t\t</div>\n",
      "\t\t\t\t<div class=\"date\">11/23</div>\n",
      "\t\t\t\t<div class=\"mark\">!</div>\n",
      "\t\t\t</div>\n",
      "\t\t</div>\n",
      "\n",
      "            \n",
      "                \n",
      "        \n",
      "        \n",
      "\t\t<div class=\"r-ent\">\n",
      "\t\t\t<div class=\"nrec\"><span class=\"hl f0\">X5</span></div>\n",
      "\t\t\t<div class=\"title\">\n",
      "\t\t\t\n",
      "\t\t\t\t<a href=\"/bbs/Beauty/M.1591064554.A.236.html\">[公告] 神記者請先搜尋板上</a>\n",
      "\t\t\t\n",
      "\t\t\t</div>\n",
      "\t\t\t<div class=\"meta\">\n",
      "\t\t\t\t<div class=\"author\">hateOnas</div>\n",
      "\t\t\t\t<div class=\"article-menu\">\n",
      "\t\t\t\t\t\n",
      "\t\t\t\t\t<div class=\"trigger\">&#x22ef;</div>\n",
      "\t\t\t\t\t<div class=\"dropdown\">\n",
      "\t\t\t\t\t\t<div class=\"item\"><a href=\"/bbs/Beauty/search?q=thread%3A%5B%E5%85%AC%E5%91%8A%5D&#43;%E7%A5%9E%E8%A8%98%E8%80%85%E8%AB%8B%E5%85%88%E6%90%9C%E5%B0%8B%E6%9D%BF%E4%B8%8A\">搜尋同標題文章</a></div>\n",
      "\t\t\t\t\t\t\n",
      "\t\t\t\t\t\t<div class=\"item\"><a href=\"/bbs/Beauty/search?q=author%3AhateOnas\">搜尋看板內 hateOnas 的文章</a></div>\n",
      "\t\t\t\t\t\t\n",
      "\t\t\t\t\t</div>\n",
      "\t\t\t\t\t\n",
      "\t\t\t\t</div>\n",
      "\t\t\t\t<div class=\"date\"> 6/02</div>\n",
      "\t\t\t\t<div class=\"mark\">M</div>\n",
      "\t\t\t</div>\n",
      "\t\t</div>\n",
      "\n",
      "            \n",
      "        \n",
      "\t</div>\n",
      "\n",
      "    \n",
      "<div class=\"bbs-screen bbs-footer-message\">本網站已依台灣網站內容分級規定處理。此區域為限制級，未滿十八歲者不得瀏覽。</div>\n",
      "\n",
      "</div>\n",
      "\n",
      "\t\t\n",
      "\n",
      "<script>\n",
      "  (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){\n",
      "  (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),\n",
      "  m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)\n",
      "  })(window,document,'script','https://www.google-analytics.com/analytics.js','ga');\n",
      "\n",
      "  ga('create', 'UA-32365737-1', {\n",
      "    cookieDomain: 'ptt.cc',\n",
      "    legacyCookieDomain: 'ptt.cc'\n",
      "  });\n",
      "  ga('send', 'pageview');\n",
      "</script>\n",
      "\n",
      "\n",
      "\t\t\n",
      "<script src=\"//ajax.googleapis.com/ajax/libs/jquery/2.1.1/jquery.min.js\"></script>\n",
      "<script src=\"//images.ptt.cc/bbs/v2.27/bbs.js\"></script>\n",
      "\n",
      "    </body>\n",
      "</html>\n",
      "\n"
     ]
    }
   ],
   "source": [
    "payload = {'from':'/bbs/Beauty/index.html', 'yes': 'yes'}\n",
    "rs = requests.session()\n",
    "res = rs.post(\"https://www.ptt.cc/ask/over18\", data=payload)\n",
    "url = \"https://www.ptt.cc/bbs/Beauty/index.html\"\n",
    "res = rs.get(url)\n",
    "# print(res.text)\n",
    "\n",
    "#print(r.url)\n",
    "#content = r.text\n",
    "#print(content)\n",
    "#soup = BeautifulSoup(content, 'html.parser')\n",
    "#print(soup.p)\n",
    "#print(soup.title)\n",
    "#print(soup.title.string)\n",
    "#print(soup.find_all('h1'))\n",
    "#print(soup.find(id=\"nav\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def craw(url):\n",
    "    all_article_file = open(\"all_articles.txt\", \"wb+\")\n",
    "    all_pupolar_file = open(\"all_popular.txt\", \"wb+\")\n",
    "\n",
    "    start_time=datatime.datatime.now()  # mark start time\n",
    "    num_page = 395          # total pages in 2019\n",
    "\n",
    "    while(num_page > 0):\n",
    "        res = requests.get(url, cookies={\"over18\": \"1\"})\n",
    "        time.sleep(0.05)\n",
    "        soup = BeautifulSoup(res.text, \"html.parser\")\n",
    "        posts = soup.find_all(class_=\"r-ent\")\n",
    "\n",
    "        for post in posts:\n",
    "            date = post.find_all(class_=\"date\")\n",
    "            day_str = date[0].string.split('/')\n",
    "            day = int(day_str[0] + day_str[1])\n",
    "            # 12/31\n",
    "            if url == \"https://www.ptt.cc/bbs/Beauty/index3142.html\" and int(day_str[0]) == 1:\n",
    "                continue\n",
    "            # 1/1\n",
    "            if url == \"https://www.ptt.cc/bbs/Beauty/index2748.html\" and int(day_str[0]) == 12:\n",
    "                continue\n",
    "            # find url link\n",
    "            link = post.find('a')\n",
    "            if link:\n",
    "                url_info = \"https://www.ptt.cc\" + link.get(\"href\")\n",
    "            else:\n",
    "                continue\n",
    "            # find title\n",
    "            title = list(link.strings)\n",
    "            # generate output line\n",
    "            output_line = str(day) + ',' + \"\".join(title) + ',' + url_info + '\\n'\n",
    "            print(output_line.rstrip())\n",
    "            # check for \"公告\" -> remove\n",
    "            if re.search(\"公告\", output_line):\n",
    "                continue\n",
    "            all_article_file.write(output_line.encode(\"utf-8\"))\n",
    "            # check for \"爆\" -> add in popular file\n",
    "            if re.search(\"爆\", str(post.find_all(\"span\"))):\n",
    "                all_pupolar_file.write(output_line.encode(\"utf-8\"))\n",
    "        \n",
    "        # go to the next page\n",
    "        url = \"https://www.ptt.cc\" + soup.find_all(class_ = \"btn wide\")[2].get(\"href\")\n",
    "        num_page = num_page - 1\n",
    "    \n",
    "    # close files\n",
    "    all_article_file.close()\n",
    "    all_pupolar_file.close()\n",
    "\n",
    "    # mark end time and calculate time consumption\n",
    "    end_time = datatime.datatime.now()\n",
    "    print(\"spent time: \", end_time-start_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def push(search_start_time, search_end_time):\n",
    "    # mark start time\n",
    "    start_time = datetime.datetime.now()\n",
    "    \n",
    "    # read data from .txt\n",
    "    all_article_file = open(\"all_articles.txt\", \"r\")\n",
    "    posts = all_article_file.readlines()\n",
    "    all_article_file.close()\n",
    "\n",
    "    push_output_file = open(\"push[%d-%d].txt\" %(search_start_time, search_end_time), \"wb+\")\n",
    "    # initialize parameters\n",
    "    like_count = 0\n",
    "    boo_count = 0\n",
    "    push_dict = {}\n",
    "\n",
    "    for post in posts:\n",
    "        select_post = post.split(',')\n",
    "        post_day = int(select_post[0])\n",
    "        # post_title = str(select_post[1])\n",
    "        post_url = str(select_post[-1]).rstrip()\n",
    "\n",
    "        if post_day > search_end_time:\n",
    "            break\n",
    "        if post_day >= search_start_time:\n",
    "            res = requests.get(post_url, cookies={\"over18\": \"1\"})\n",
    "            time.sleep(0.05)\n",
    "            soup = BeautifulSoup(res.text, \"html.parser\")    \n",
    "            all_push = soup.find_all(class_=\"push\")\n",
    "            # *** !!! ***\n",
    "            check_exist = soup.find(class_=\"bbs-screen bbs-content\")\n",
    "            if check_exist:\n",
    "                content = check_exist.text\n",
    "            else:\n",
    "                content = \"N/A\"\n",
    "            end = \"--\"\n",
    "            check = re.search(end, content)\n",
    "\n",
    "            if check:\n",
    "                for push in all_push:\n",
    "                    push_info = push.find_all(\"span\")\n",
    "                    if len(push_info) != 0:\n",
    "                        # tag = push_info[0].string\n",
    "                        user_id = push_info[1].string\n",
    "\n",
    "                    if re.search(\"推\", str(push_info)):\n",
    "                        like_count += 1\n",
    "                        if user_id in push_dict:\n",
    "                            push_dict[user_id][\"like\"] += 1\n",
    "                        else:\n",
    "                            push_dict[user_id] = {\"like\": 1, \"boo\": 0}\n",
    "\n",
    "                    if re.search(\"噓\", str(push_info)):\n",
    "                        boo_count += 1\n",
    "                        if user_id in push_dict:\n",
    "                            push_dict[user_id][\"boo\"] += 1\n",
    "                        else:\n",
    "                            push_dict[user_id] = {\"like\": 0, \"boo\": 1}\n",
    "\n",
    "            else:\n",
    "                print(\"there is no \\\"--\\\" in \", post_url)\n",
    "                continue\n",
    "\n",
    "    output_line = []\n",
    "    output_line.append(\"all like: %d\\n\" %like_count)\n",
    "    output_line.append(\"all boo: %d\\n\" %boo_count)\n",
    "    \n",
    "    like_rank = sorted(push_dict, key=lambda x: (push_dict[x][\"like\"]*-1, x), reverse=True)\n",
    "    for i, j in enumerate(reversed(like_rank[-10:])):\n",
    "        temp = \"like #%d: %s %d\\n\" %((i+1), j, push_dict[j][\"like\"])\n",
    "        # print(temp.rstrip())\n",
    "        output_line.append(temp)\n",
    "\n",
    "    boo_rank = sorted(push_dict, key=lambda x: (push_dict[x][\"boo\"]*-1, x), reverse=True)\n",
    "    for i, j in enumerate(reversed(boo_rank[-10:])):\n",
    "        temp = \"boo #%d: %s %d\\n\" %((i+1), j, push_dict[j][\"boo\"])\n",
    "        # print(temp.rstrip())\n",
    "        output_line.append(temp)\n",
    "\n",
    "    output_line = \"\".join(output_line) + '\\n'\n",
    "    push_output_file.write((output_line).encode('utf-8'))\n",
    "    push_output_file.close()\n",
    "\n",
    "    # mark end time and calculate time consumption\n",
    "    end_time = datetime.datetime.now()\n",
    "    print(\"spent time: \", end_time-start_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def popular(search_start_time, search_end_time):\n",
    "    # mark start time\n",
    "    start_time = datetime.datetime.now()\n",
    "    \n",
    "    # read data from .txt\n",
    "    all_popular_file = open(\"all_popular.txt\", \"r\")\n",
    "    popular_posts = all_popular_file.readlines()\n",
    "    all_popular_file.close()\n",
    "\n",
    "    popular_output_file = open(\"popular[%d-%d].txt\" %(search_start_time, search_end_time), \"wb+\")\n",
    "    # initialize parameters\n",
    "    popular_number = 0\n",
    "    output_line = []\n",
    "    output_temp = []\n",
    "    img_url = []\n",
    "\n",
    "    for post in popular_posts:\n",
    "        select_post = post.split(',')\n",
    "        post_day = int(select_post[0])\n",
    "        # post_title = str(select_post[1])\n",
    "        post_url = str(select_post[-1]).rstrip()\n",
    "\n",
    "        if post_day > search_end_time:\n",
    "            break\n",
    "        if post_day >= search_start_time:\n",
    "            popular_number += 1\n",
    "\n",
    "            try:\n",
    "                res = requests.get(post_url, cookies={\"over18\": \"1\"})\n",
    "            except Exception as e: print(e)\n",
    "            time.sleep(0.05)\n",
    "            soup = BeautifulSoup(res.text, \"html.parser\")    \n",
    "            # *** !!! ***\n",
    "            check_exist = soup.find(class_=\"bbs-screen bbs-content\")\n",
    "            if check_exist:\n",
    "                content = check_exist.text\n",
    "            else:\n",
    "                content = \"N/A\"\n",
    "            end = \"--\"\n",
    "            check = re.search(end, content)\n",
    "\n",
    "            if check:\n",
    "                img_url_pattern = 'href=\"(http|https)(.*)?(jpg|jpeg|png|gif)'\n",
    "                img_url = re.findall(img_url_pattern, soup.prettify())\n",
    "                for string in img_url:\n",
    "                    output_temp.append(\"\".join(string) + '\\n')\n",
    "            else:\n",
    "                print(\"no image url in \\\"%s\\\"\" %(post_url))\n",
    "                continue\n",
    "\n",
    "    output_line.append(\"number of popular articles: %d\\n\" %(popular_number))\n",
    "    output_line.append(\"\".join(output_temp) + '\\n')\n",
    "    output_line = \"\".join(output_line) + '\\n'\n",
    "    popular_output_file.write((output_line).encode('utf-8'))\n",
    "    popular_output_file.close()\n",
    "\n",
    "    # mark end time and calculate time consumption\n",
    "    end_time = datetime.datetime.now()\n",
    "    print(\"spent time: \", end_time-start_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def keyword(key_word, search_start_time, search_end_time):\n",
    "    # mark start time\n",
    "    start_time = datetime.datetime.now()\n",
    "    \n",
    "    # read data from .txt\n",
    "    all_article_file = open(\"all_articles.txt\", \"r\")\n",
    "    posts = all_article_file.readlines()\n",
    "    all_article_file.close()\n",
    "\n",
    "    keyword_output_file = open(\"push[%d-%d].txt\" %(search_start_time, search_end_time), \"wb+\")\n",
    "    # initialize parameters\n",
    "    output_line = []\n",
    "    output_temp = []\n",
    "\n",
    "    for post in posts:\n",
    "        select_post = post.split(',')\n",
    "        post_day = int(select_post[0])\n",
    "        # post_title = str(select_post[1])\n",
    "        post_url = str(select_post[-1]).rstrip()\n",
    "\n",
    "        if post_day > search_end_time:\n",
    "            break\n",
    "        if post_day >= search_start_time:\n",
    "            try:\n",
    "                res = requests.get(post_url, cookies={\"over18\": \"1\"})\n",
    "            except Exception as e: print(e)\n",
    "            time.sleep(0.05)\n",
    "            soup = BeautifulSoup(res.text, \"html.parser\")    \n",
    "            # *** !!! ***\n",
    "            check_exist = soup.find(class_=\"bbs-screen bbs-content\")\n",
    "            if check_exist:\n",
    "                content = check_exist.text\n",
    "            else:\n",
    "                content = \"N/A\"\n",
    "            end = \"--\"\n",
    "            check = re.search(end, content)\n",
    "\n",
    "            if check:\n",
    "                content_list = content.split('\\n')\n",
    "\n",
    "                for match in content_list:\n",
    "                    if re.search(end, match):\n",
    "                        break\n",
    "                    else:\n",
    "                        if re.search(key_word, match):\n",
    "                            print(\"Find!\", post_url)\n",
    "                            img_url_pattern = 'href=\"(http|https)(.*)?(jpg|jpeg|png|gif)'\n",
    "                            img_url = re.findall(img_url_pattern, soup.prettify())\n",
    "                            for string in img_url:\n",
    "                                output_temp.append(\"\".join(string) + '\\n')\n",
    "            else:\n",
    "                print(\"no \\\"--\\\" in \\\"%s\\\"\" %(post_url))\n",
    "                continue\n",
    "\n",
    "    output_line.append(\"\".join(output_temp) + '\\n')\n",
    "    output_line = \"\".join(output_line) + '\\n'\n",
    "    keyword_output_file.write((output_line).encode('utf-8'))\n",
    "    keyword_output_file.close()\n",
    "\n",
    "    # mark end time and calculate time consumption\n",
    "    end_time = datetime.datetime.now()\n",
    "    print(\"spent time: \", end_time-start_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def craw_img(search_start_time, search_end_time):\n",
    "    # mark start time\n",
    "    start_time = datetime.datetime.now()\n",
    "    \n",
    "    # read data from .txt\n",
    "    all_article_file = open(\"all_articles.txt\", \"r\")\n",
    "    posts = all_article_file.readlines()\n",
    "    all_article_file.close()\n",
    "\n",
    "    img_url_output_file = open(\"article_img_url[%d-%d].txt\" %(search_start_time, search_end_time), \"wb+\")\n",
    "    # initialize parameters\n",
    "    article_number = 0\n",
    "    output_line = []\n",
    "    output_temp = []\n",
    "    img_url = []\n",
    "\n",
    "    for post in posts:\n",
    "        select_post = post.split(',')\n",
    "        post_day = int(select_post[0])\n",
    "        # post_title = str(select_post[1])\n",
    "        post_url = str(select_post[-1]).rstrip()\n",
    "\n",
    "        if post_day > search_end_time:\n",
    "            break\n",
    "        if post_day >= search_start_time:\n",
    "            article_number += 1\n",
    "\n",
    "            try:\n",
    "                res = requests.get(post_url, cookies={\"over18\": \"1\"})\n",
    "            except Exception as e: print(e)\n",
    "            time.sleep(0.05)\n",
    "            soup = BeautifulSoup(res.text, \"html.parser\")    \n",
    "            # *** !!! ***\n",
    "            check_exist = soup.find(class_=\"bbs-screen bbs-content\")\n",
    "            if check_exist:\n",
    "                content = check_exist.text\n",
    "            else:\n",
    "                content = \"N/A\"\n",
    "            end = \"※ 發信站\"\n",
    "            check = re.search(end, content)\n",
    "\n",
    "            if check:\n",
    "                img_url_pattern = 'href=\"(http|https)(.*)?(jpg|jpeg|png|gif)'\n",
    "                img_url = re.findall(img_url_pattern, soup.prettify())\n",
    "                for string in img_url:\n",
    "                    output_temp.append(\"\".join(string) + '\\n')\n",
    "            else:\n",
    "                print(\"no \\\"發信站\\\" in \\\"%s\\\"\" %(post_url))\n",
    "                continue\n",
    "\n",
    "    output_line.append(\"number of articles: %d\\n\" %(article_number))\n",
    "    output_line.append(\"\".join(output_temp) + '\\n')\n",
    "    output_line = \"\".join(output_line) + '\\n'\n",
    "    img_url_output_file.write((output_line).encode('utf-8'))\n",
    "    img_url_output_file.close()\n",
    "\n",
    "    # mark end time and calculate time consumption\n",
    "    end_time = datetime.datetime.now()\n",
    "    print(\"spent time: \", end_time-start_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "crawl image url from 101 to 1231\n",
      "no \"發信站\" in \"https://www.ptt.cc/bbs/Beauty/M.1548031133.A.0CC.html\"\n",
      "no \"發信站\" in \"https://www.ptt.cc/bbs/Beauty/M.1548648937.A.BE4.html\"\n",
      "no \"發信站\" in \"https://www.ptt.cc/bbs/Beauty/M.1549193784.A.B20.html\"\n",
      "no \"發信站\" in \"https://www.ptt.cc/bbs/Beauty/M.1549237932.A.566.html\"\n",
      "no \"發信站\" in \"https://www.ptt.cc/bbs/Beauty/M.1549787561.A.31D.html\"\n",
      "no \"發信站\" in \"https://www.ptt.cc/bbs/Beauty/M.1549974705.A.611.html\"\n",
      "no \"發信站\" in \"https://www.ptt.cc/bbs/Beauty/M.1554205932.A.CEC.html\"\n",
      "no \"發信站\" in \"https://www.ptt.cc/bbs/Beauty/M.1555077384.A.9D8.html\"\n",
      "no \"發信站\" in \"https://www.ptt.cc/bbs/Beauty/M.1555293023.A.A36.html\"\n",
      "no \"發信站\" in \"https://www.ptt.cc/bbs/Beauty/M.1555415158.A.9D9.html\"\n",
      "no \"發信站\" in \"https://www.ptt.cc/bbs/Beauty/M.1555421795.A.2F1.html\"\n",
      "no \"發信站\" in \"https://www.ptt.cc/bbs/Beauty/M.1557676878.A.765.html\"\n",
      "no \"發信站\" in \"https://www.ptt.cc/bbs/Beauty/M.1559495398.A.F36.html\"\n",
      "no \"發信站\" in \"https://www.ptt.cc/bbs/Beauty/M.1565503042.A.AB5.html\"\n",
      "no \"發信站\" in \"https://www.ptt.cc/bbs/Beauty/M.1566438606.A.E1B.html\"\n",
      "no \"發信站\" in \"https://www.ptt.cc/bbs/Beauty/M.1567409693.A.071.html\"\n",
      "no \"發信站\" in \"https://www.ptt.cc/bbs/Beauty/M.1573480353.A.B2A.html\"\n",
      "no \"發信站\" in \"https://www.ptt.cc/bbs/Beauty/M.1574176139.A.194.html\"\n",
      "no \"發信站\" in \"https://www.ptt.cc/bbs/Beauty/M.1574544165.A.F61.html\"\n",
      "no \"發信站\" in \"https://www.ptt.cc/bbs/Beauty/M.1575787775.A.609.html\"\n",
      "no \"發信站\" in \"https://www.ptt.cc/bbs/Beauty/M.1576395252.A.477.html\"\n",
      "no \"發信站\" in \"https://www.ptt.cc/bbs/Beauty/M.1576828103.A.DCC.html\"\n",
      "spent time:  1:50:11.103489\n"
     ]
    }
   ],
   "source": [
    "#print(\"2019 Beauty crawler\") \n",
    "#url = \"https://www.ptt.cc/bbs/Beauty/index2748.html\"\n",
    "#crawl(url)\n",
    "\n",
    "#print(\"crawl push and boo from\", int(args.mod[1]), \"to\", int(args.mod[2]))\n",
    "#print(\"crawl push and boo from\", 101, \"to\", 1231)\n",
    "#push(int(args.mod[1]),int(args.mod[2]))\n",
    "#push(101,1231)\n",
    "        \n",
    "#print(\"crawl popular from\", int(args.mod[1]), \"to\", int(args.mod[2]))\n",
    "#print(\"crawl popular from\", 101, \"to\", 1231)\n",
    "#popular(int(args.mod[1]),int(args.mod[2]))\n",
    "#popular(101,1231)\n",
    "\n",
    "#print(\"crawl and search\",args.mod[1], \"from\", int(args.mod[2]), \"to\", int(args.mod[3]))\n",
    "#print(\"crawl and search\", \"波多\", \"from\", 101, \"to\", 1231)\n",
    "#keyword(str(args.mod[1]),int(args.mod[2]),int(args.mod[3]))\n",
    "#keyword(\"波多\", 101, 1231)\n",
    "\n",
    "#print(\"crawl image url from\", int(args.mod[1]), \"to\", int(args.mod[2]))\n",
    "print(\"crawl image url from\", 101, \"to\", 1231)\n",
    "#keyword(int(args.mod[1]), int(args.mod[2]))\n",
    "craw_img(101, 1231)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
